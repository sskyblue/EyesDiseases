{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining = False\n",
    "Batch_size = 16\n",
    "\n",
    "if pretraining:\n",
    "    epoch = 30\n",
    "    learning_rate = 0.001\n",
    "else:\n",
    "    epoch = 50\n",
    "    learning_rate = 0.002\n",
    "\n",
    "net = [0] * 5\n",
    "for type in range(5):\n",
    "    train_dataset = cDataset(path, train = True, type = type, transform = train_transforms)\n",
    "    val_dataset = cDataset(path, train = False, type = type, transform = valid_transforms)\n",
    "\n",
    "    train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                        batch_size=Batch_size,\n",
    "                        shuffle=True,\n",
    "                        drop_last=False)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset,\n",
    "                        batch_size=Batch_size,\n",
    "                        shuffle=True,\n",
    "                        drop_last=False)\n",
    "\n",
    "    print('Training dataset size:', len(train_dataset))\n",
    "    print('Validation dataset size:', len(val_dataset))\n",
    "\n",
    "    # 모델 바꾸는 부분\n",
    "    net[type] = torchvision.models.densenet161(num_classes = 2,pretrained = pretraining)\n",
    "    net[type] = net[type].to(device)\n",
    "    file_name = \"densenet161\" + str(type) + \".pt\" \n",
    "    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min') # 스케줄링 여부에 따라 달라짐
    "    # 모델 바꾸는 부분\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net[type].parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0002)\n",
    "\n",
    "    train_result = []\n",
    "    val_result = []\n",
    "\n",
    "    start_time = time.time() # 시작 시간\n",
    "    max_val_acc = -1\n",
    "    for i in range(epoch):\n",
    "        train_acc, train_loss = train(net[type], i + 1, optimizer, criterion, train_dataloader) # 학습(training)\n",
    "        val_acc, val_loss = validate(net[type], i + 1, criterion, val_dataloader) # 검증(validation)\n",
    "        \n",
    "        # 학습된 모델 저장하기\n",
    "        state = {\n",
    "            'net': net[type].state_dict()\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        if val_acc > max_val_acc:\n",
    "            max_val_acc = val_acc\n",
    "            torch.save(state, './checkpoint/' + file_name)\n",
    "            print(f'Model saved! (time elapsed: {time.time() - start_time})')\n",
    "\n",
    "        # 현재 epoch에서의 정확도(accuracy)와 손실(loss) 값 저장하기\n",
    "        train_result.append((train_acc, train_loss))\n",
    "        val_result.append((val_acc, val_loss))\n",
    "\n",
    "    # 평가 데이터셋을 이용해 혼동 행렬(confusion matrix) 계산하기\n",
    "    \n",
    "    checkpoint = torch.load('./checkpoint/' + file_name)\n",
    "    net[type].load_state_dict(checkpoint['net'])\n",
    "    \n",
    "    confusion_matrix = get_confusion_matrix(net[type], 2, val_dataloader)\n",
    "    print(\"[ 각 클래스당 데이터 개수 ]\")\n",
    "    print(confusion_matrix.sum(1))\n",
    "\n",
    "    print(\"[ 혼동 행렬(confusion matrix) 시각화 ]\")\n",
    "    res = pd.DataFrame(confusion_matrix.numpy(), index = [i for i in range(2)], columns = [i for i in range(2)])\n",
    "    res.index.name = 'True label'\n",
    "    res.columns.name = 'Predicted label'\n",
    "    plt.figure(figsize = (10, 7))\n",
    "    sns.heatmap(res, annot=True, fmt=\"d\", cmap='Blues')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"[ 각 클래스에 따른 정확도 ]\")\n",
    "    # (각 클래스마다 정답 개수 / 각 클래스마다 데이터의 개수)\n",
    "    print(confusion_matrix.diag() / confusion_matrix.sum(1))\n",
    "\n",
    "    print(\"[ 전체 평균 정확도 ]\")\n",
    "    print(confusion_matrix.diag().sum() / confusion_matrix.sum())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
